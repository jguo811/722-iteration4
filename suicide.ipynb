{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('tree_methods_adv').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data. \n",
    "data = spark.read.csv('Datasets/suicide rate.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- suicides_no: integer (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      " |-- suicides/100k pop: double (nullable = true)\n",
      " |-- country-year: string (nullable = true)\n",
      " |-- HDI for year: double (nullable = true)\n",
      " |--  gdp_for_year ($) : string (nullable = true)\n",
      " |-- gdp_per_capita ($): integer (nullable = true)\n",
      " |-- generation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's get an idea of what the data looks like. \n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- suicides_no: integer (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      " |-- suicide_rate: double (nullable = true)\n",
      " |-- country-year: string (nullable = true)\n",
      " |-- HDI: double (nullable = true)\n",
      " |-- gdp_year: string (nullable = true)\n",
      " |-- gdp_capita: integer (nullable = true)\n",
      " |-- generation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumnRenamed('suicides/100k pop','suicide_rate')\n",
    "data = data.withColumnRenamed(' gdp_for_year ($) ','gdp_year')\n",
    "data = data.withColumnRenamed('gdp_per_capita ($)','gdp_capita')\n",
    "data = data.withColumnRenamed('HDI for year','HDI')\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('suicides_no','country-year','population')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---+---+------------+-----+--------+----------+----------+\n",
      "|country|year|sex|age|suicide_rate|  HDI|gdp_year|gdp_capita|generation|\n",
      "+-------+----+---+---+------------+-----+--------+----------+----------+\n",
      "|      0|   0|  0|  0|           0|19456|       0|         0|         0|\n",
      "+-------+----+---+---+------------+-----+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_agg = data.agg(*[F.count(F.when(F.isnull(c),c)).alias(c) for c in data.columns])\n",
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- suicide_rate: double (nullable = true)\n",
      " |-- gdp_year: string (nullable = true)\n",
      " |-- gdp_capita: integer (nullable = true)\n",
      " |-- generation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.drop('HDI')\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop('year','generation')\n",
    "# data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+-----------+------------+-------------+----------+---------------+------------+\n",
      "|country|year|   sex|        age|suicide_rate|     gdp_year|gdp_capita|     generation|suicide_risk|\n",
      "+-------+----+------+-----------+------------+-------------+----------+---------------+------------+\n",
      "|Albania|1987|  male|15-24 years|        6.71|2,156,624,900|       796|   Generation X|           0|\n",
      "|Albania|1987|  male|35-54 years|        5.19|2,156,624,900|       796|         Silent|           0|\n",
      "|Albania|1987|female|15-24 years|        4.83|2,156,624,900|       796|   Generation X|           0|\n",
      "|Albania|1987|  male|  75+ years|        4.59|2,156,624,900|       796|G.I. Generation|           0|\n",
      "|Albania|1987|  male|25-34 years|        3.28|2,156,624,900|       796|        Boomers|           0|\n",
      "|Albania|1987|female|  75+ years|        2.81|2,156,624,900|       796|G.I. Generation|           0|\n",
      "|Albania|1987|female|35-54 years|        2.15|2,156,624,900|       796|         Silent|           0|\n",
      "|Albania|1987|female|25-34 years|        1.56|2,156,624,900|       796|        Boomers|           0|\n",
      "|Albania|1987|  male|55-74 years|        0.73|2,156,624,900|       796|G.I. Generation|           0|\n",
      "|Albania|1987|female|05-14 years|         0.0|2,156,624,900|       796|   Generation X|           0|\n",
      "|Albania|1987|female|55-74 years|         0.0|2,156,624,900|       796|G.I. Generation|           0|\n",
      "|Albania|1987|  male|05-14 years|         0.0|2,156,624,900|       796|   Generation X|           0|\n",
      "|Albania|1988|female|  75+ years|        5.49|2,126,000,000|       769|G.I. Generation|           0|\n",
      "|Albania|1988|  male|15-24 years|        5.33|2,126,000,000|       769|   Generation X|           0|\n",
      "|Albania|1988|  male|  75+ years|        4.48|2,126,000,000|       769|G.I. Generation|           0|\n",
      "|Albania|1988|  male|35-54 years|        4.46|2,126,000,000|       769|         Silent|           0|\n",
      "|Albania|1988|  male|55-74 years|        2.85|2,126,000,000|       769|G.I. Generation|           0|\n",
      "|Albania|1988|female|15-24 years|        2.71|2,126,000,000|       769|   Generation X|           0|\n",
      "|Albania|1988|female|55-74 years|        2.03|2,126,000,000|       769|G.I. Generation|           0|\n",
      "|Albania|1988|female|25-34 years|        1.91|2,126,000,000|       769|        Boomers|           0|\n",
      "+-------+----+------+-----------+------------+-------------+----------+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "data = data.withColumn(\"suicide_risk\",F.when(data.suicide_rate> 12.816097411933894, 1 ).otherwise(0))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- suicide_rate: double (nullable = true)\n",
      " |-- gdp_capita: integer (nullable = true)\n",
      " |-- generation: string (nullable = true)\n",
      " |-- suicide_risk: integer (nullable = false)\n",
      " |-- gdp_year: float (nullable = true)\n",
      "\n",
      "+-------+----+------+-----------+------------+----------+---------------+------------+-----------+\n",
      "|country|year|   sex|        age|suicide_rate|gdp_capita|     generation|suicide_risk|   gdp_year|\n",
      "+-------+----+------+-----------+------------+----------+---------------+------------+-----------+\n",
      "|Albania|1987|  male|15-24 years|        6.71|       796|   Generation X|           0|2.1566249E9|\n",
      "|Albania|1987|  male|35-54 years|        5.19|       796|         Silent|           0|2.1566249E9|\n",
      "|Albania|1987|female|15-24 years|        4.83|       796|   Generation X|           0|2.1566249E9|\n",
      "|Albania|1987|  male|  75+ years|        4.59|       796|G.I. Generation|           0|2.1566249E9|\n",
      "|Albania|1987|  male|25-34 years|        3.28|       796|        Boomers|           0|2.1566249E9|\n",
      "|Albania|1987|female|  75+ years|        2.81|       796|G.I. Generation|           0|2.1566249E9|\n",
      "|Albania|1987|female|35-54 years|        2.15|       796|         Silent|           0|2.1566249E9|\n",
      "|Albania|1987|female|25-34 years|        1.56|       796|        Boomers|           0|2.1566249E9|\n",
      "|Albania|1987|  male|55-74 years|        0.73|       796|G.I. Generation|           0|2.1566249E9|\n",
      "|Albania|1987|female|05-14 years|         0.0|       796|   Generation X|           0|2.1566249E9|\n",
      "|Albania|1987|female|55-74 years|         0.0|       796|G.I. Generation|           0|2.1566249E9|\n",
      "|Albania|1987|  male|05-14 years|         0.0|       796|   Generation X|           0|2.1566249E9|\n",
      "|Albania|1988|female|  75+ years|        5.49|       769|G.I. Generation|           0|    2.126E9|\n",
      "|Albania|1988|  male|15-24 years|        5.33|       769|   Generation X|           0|    2.126E9|\n",
      "|Albania|1988|  male|  75+ years|        4.48|       769|G.I. Generation|           0|    2.126E9|\n",
      "|Albania|1988|  male|35-54 years|        4.46|       769|         Silent|           0|    2.126E9|\n",
      "|Albania|1988|  male|55-74 years|        2.85|       769|G.I. Generation|           0|    2.126E9|\n",
      "|Albania|1988|female|15-24 years|        2.71|       769|   Generation X|           0|    2.126E9|\n",
      "|Albania|1988|female|55-74 years|        2.03|       769|G.I. Generation|           0|    2.126E9|\n",
      "|Albania|1988|female|25-34 years|        1.91|       769|        Boomers|           0|    2.126E9|\n",
      "+-------+----+------+-----------+------------+----------+---------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, concat, col, lit\n",
    "commaRep = udf(lambda x: x.replace(',','')) \n",
    "data = data.withColumn('gdp_years',commaRep('gdp_year'))\n",
    "data = data.drop('gdp_year')\n",
    "from pyspark.sql.types import IntegerType\n",
    "# data = data.withColumn(\"gdp_year\", data[\"gdp_years\"].cast(IntegerType()))\n",
    "data = data.withColumn(\"gdp_year\", data.gdp_years.cast('float'))\n",
    "data = data.drop('gdp_years')\n",
    "data.printSchema()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the string indexer (similar to the logistic regression exercises).\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "Year_Indexer = StringIndexer(inputCol='year',outputCol='yearIndex')\n",
    "Year_Indexed = Year_Indexer.fit(data).transform(data)\n",
    "\n",
    "Sex_Indexer = StringIndexer(inputCol='sex',outputCol='sexIndex')\n",
    "Sex_Indexed = Sex_Indexer.fit(Year_Indexed).transform(Year_Indexed)\n",
    "\n",
    "Country_Indexer = StringIndexer(inputCol='country',outputCol='countryIndex')\n",
    "Country_Indexed = Country_Indexer.fit(Sex_Indexed).transform(Sex_Indexed)\n",
    "\n",
    "Age_Indexer = StringIndexer(inputCol='age',outputCol='ageIndex')\n",
    "Age_Indexed = Age_Indexer.fit(Country_Indexed).transform(Country_Indexed)\n",
    "\n",
    "Generation_Indexer = StringIndexer(inputCol='generation',outputCol='generationIndex')\n",
    "Generation_Indexed = Generation_Indexer.fit(Age_Indexed).transform(Age_Indexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gdp_capita: integer (nullable = true)\n",
      " |-- suicide_risk: integer (nullable = false)\n",
      " |-- gdp_year: float (nullable = true)\n",
      " |-- yearIndex: double (nullable = true)\n",
      " |-- sexIndex: double (nullable = true)\n",
      " |-- countryIndex: double (nullable = true)\n",
      " |-- ageIndex: double (nullable = true)\n",
      " |-- generationIndex: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Generation_Indexed = Generation_Indexed.drop('country','year','sex','age','suicide_rate','generation')\n",
    "Generation_Indexed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "  inputCols=[\n",
    "             'countryIndex',\n",
    "#               'yearIndex',\n",
    "              'sexIndex',\n",
    "              'gdp_year',\n",
    "             'gdp_capita',\n",
    "             'ageIndex',\n",
    "             'generationIndex'\n",
    "             ],\n",
    "              outputCol=\"features\")\n",
    "output = assembler.transform(Generation_Indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select(\"features\",'suicide_risk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+------------+\n",
      "|features                              |suicide_risk|\n",
      "+--------------------------------------+------------+\n",
      "|[67.0,0.0,2.156624896E9,796.0,2.0,0.0]|0           |\n",
      "|[67.0,0.0,2.156624896E9,796.0,0.0,1.0]|0           |\n",
      "|[67.0,1.0,2.156624896E9,796.0,2.0,0.0]|0           |\n",
      "|[67.0,0.0,2.156624896E9,796.0,4.0,4.0]|0           |\n",
      "|[67.0,0.0,2.156624896E9,796.0,1.0,3.0]|0           |\n",
      "|[67.0,1.0,2.156624896E9,796.0,4.0,4.0]|0           |\n",
      "|[67.0,1.0,2.156624896E9,796.0,0.0,1.0]|0           |\n",
      "|[67.0,1.0,2.156624896E9,796.0,1.0,3.0]|0           |\n",
      "|[67.0,0.0,2.156624896E9,796.0,3.0,4.0]|0           |\n",
      "|[67.0,1.0,2.156624896E9,796.0,5.0,0.0]|0           |\n",
      "|[67.0,1.0,2.156624896E9,796.0,3.0,4.0]|0           |\n",
      "|[67.0,0.0,2.156624896E9,796.0,5.0,0.0]|0           |\n",
      "|[67.0,1.0,2.126E9,769.0,4.0,4.0]      |0           |\n",
      "|[67.0,0.0,2.126E9,769.0,2.0,0.0]      |0           |\n",
      "|[67.0,0.0,2.126E9,769.0,4.0,4.0]      |0           |\n",
      "|[67.0,0.0,2.126E9,769.0,0.0,1.0]      |0           |\n",
      "|[67.0,0.0,2.126E9,769.0,3.0,4.0]      |0           |\n",
      "|[67.0,1.0,2.126E9,769.0,2.0,0.0]      |0           |\n",
      "|[67.0,1.0,2.126E9,769.0,3.0,4.0]      |0           |\n",
      "|[67.0,1.0,2.126E9,769.0,1.0,3.0]      |0           |\n",
      "+--------------------------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.000000, 1.000000]\n",
      "+--------------------+--------------------+\n",
      "|            features|      scaledFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[67.0,0.0,2.15662...|[0.67,0.0,1.16425...|\n",
      "|[67.0,0.0,2.15662...|[0.67,0.0,1.16425...|\n",
      "|[67.0,1.0,2.15662...|[0.67,1.0,1.16425...|\n",
      "|[67.0,0.0,2.15662...|[0.67,0.0,1.16425...|\n",
      "|[67.0,0.0,2.15662...|[0.67,0.0,1.16425...|\n",
      "|[67.0,1.0,2.15662...|[0.67,1.0,1.16425...|\n",
      "|[67.0,1.0,2.15662...|[0.67,1.0,1.16425...|\n",
      "|[67.0,1.0,2.15662...|[0.67,1.0,1.16425...|\n",
      "|[67.0,0.0,2.15662...|[0.67,0.0,1.16425...|\n",
      "|[67.0,1.0,2.15662...|[0.67,1.0,1.16425...|\n",
      "|[67.0,1.0,2.15662...|[0.67,1.0,1.16425...|\n",
      "|[67.0,0.0,2.15662...|[0.67,0.0,1.16425...|\n",
      "|[67.0,1.0,2.126E9...|[0.67,1.0,1.14735...|\n",
      "|[67.0,0.0,2.126E9...|[0.67,0.0,1.14735...|\n",
      "|[67.0,0.0,2.126E9...|[0.67,0.0,1.14735...|\n",
      "|[67.0,0.0,2.126E9...|[0.67,0.0,1.14735...|\n",
      "|[67.0,0.0,2.126E9...|[0.67,0.0,1.14735...|\n",
      "|[67.0,1.0,2.126E9...|[0.67,1.0,1.14735...|\n",
      "|[67.0,1.0,2.126E9...|[0.67,1.0,1.14735...|\n",
      "|[67.0,1.0,2.126E9...|[0.67,1.0,1.14735...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(final_data)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaledData = scalerModel.transform(final_data)\n",
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n",
      "+--------------------------------------+------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|features                              |suicide_risk|normFeatures                                                                                                   |\n",
      "+--------------------------------------+------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|[67.0,0.0,2.156624896E9,796.0,2.0,0.0]|0           |[3.1067049838509276E-8,0.0,0.999999598910476,3.6909509957393115E-7,9.273746220450531E-10,0.0]                  |\n",
      "|[67.0,0.0,2.156624896E9,796.0,0.0,1.0]|0           |[3.106704985291467E-8,0.0,0.999999599374163,3.6909509974507586E-7,0.0,4.6368731123753244E-10]                  |\n",
      "|[67.0,1.0,2.156624896E9,796.0,2.0,0.0]|0           |[3.106704982410388E-8,4.6368731080752064E-10,0.9999995984467889,3.690950994027864E-7,9.273746216150413E-10,0.0]|\n",
      "|[67.0,0.0,2.156624896E9,796.0,4.0,4.0]|0           |[3.1067049752076896E-8,0.0,0.9999995961283532,3.6909509854706284E-7,1.854749238929964E-9,1.854749238929964E-9] |\n",
      "|[67.0,0.0,2.156624896E9,796.0,1.0,3.0]|0           |[3.106704980969848E-8,0.0,0.9999995979831017,3.690950992316417E-7,4.636873105925147E-10,1.3910619317775442E-9] |\n",
      "+--------------------------------------+------------+---------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(final_data)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(6, {0: 0.3461, 1: 0.3321, 2: 0.0183, 3: 0.0092, 4: 0.1839, 5: 0.1104})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(labelCol='suicide_risk',featuresCol='features',maxBins=120)\n",
    "rfc_model = rfc.fit(final_data)\n",
    "rfc_model.featureImportances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 5 features selected\n",
      "+--------------------------------------+------------+----------------------------------+\n",
      "|features                              |suicide_risk|selectedFeatures                  |\n",
      "+--------------------------------------+------------+----------------------------------+\n",
      "|[67.0,0.0,2.156624896E9,796.0,2.0,0.0]|0           |[67.0,0.0,2.156624896E9,796.0,2.0]|\n",
      "|[67.0,0.0,2.156624896E9,796.0,0.0,1.0]|0           |[67.0,0.0,2.156624896E9,796.0,0.0]|\n",
      "|[67.0,1.0,2.156624896E9,796.0,2.0,0.0]|0           |[67.0,1.0,2.156624896E9,796.0,2.0]|\n",
      "|[67.0,0.0,2.156624896E9,796.0,4.0,4.0]|0           |[67.0,0.0,2.156624896E9,796.0,4.0]|\n",
      "|[67.0,0.0,2.156624896E9,796.0,1.0,3.0]|0           |[67.0,0.0,2.156624896E9,796.0,1.0]|\n",
      "|[67.0,1.0,2.156624896E9,796.0,4.0,4.0]|0           |[67.0,1.0,2.156624896E9,796.0,4.0]|\n",
      "|[67.0,1.0,2.156624896E9,796.0,0.0,1.0]|0           |[67.0,1.0,2.156624896E9,796.0,0.0]|\n",
      "|[67.0,1.0,2.156624896E9,796.0,1.0,3.0]|0           |[67.0,1.0,2.156624896E9,796.0,1.0]|\n",
      "|[67.0,0.0,2.156624896E9,796.0,3.0,4.0]|0           |[67.0,0.0,2.156624896E9,796.0,3.0]|\n",
      "|[67.0,1.0,2.156624896E9,796.0,5.0,0.0]|0           |[67.0,1.0,2.156624896E9,796.0,5.0]|\n",
      "|[67.0,1.0,2.156624896E9,796.0,3.0,4.0]|0           |[67.0,1.0,2.156624896E9,796.0,3.0]|\n",
      "|[67.0,0.0,2.156624896E9,796.0,5.0,0.0]|0           |[67.0,0.0,2.156624896E9,796.0,5.0]|\n",
      "|[67.0,1.0,2.126E9,769.0,4.0,4.0]      |0           |[67.0,1.0,2.126E9,769.0,4.0]      |\n",
      "|[67.0,0.0,2.126E9,769.0,2.0,0.0]      |0           |[67.0,0.0,2.126E9,769.0,2.0]      |\n",
      "|[67.0,0.0,2.126E9,769.0,4.0,4.0]      |0           |[67.0,0.0,2.126E9,769.0,4.0]      |\n",
      "|[67.0,0.0,2.126E9,769.0,0.0,1.0]      |0           |[67.0,0.0,2.126E9,769.0,0.0]      |\n",
      "|[67.0,0.0,2.126E9,769.0,3.0,4.0]      |0           |[67.0,0.0,2.126E9,769.0,3.0]      |\n",
      "|[67.0,1.0,2.126E9,769.0,2.0,0.0]      |0           |[67.0,1.0,2.126E9,769.0,2.0]      |\n",
      "|[67.0,1.0,2.126E9,769.0,3.0,4.0]      |0           |[67.0,1.0,2.126E9,769.0,3.0]      |\n",
      "|[67.0,1.0,2.126E9,769.0,1.0,3.0]      |0           |[67.0,1.0,2.126E9,769.0,1.0]      |\n",
      "+--------------------------------------+------------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "selector = ChiSqSelector(numTopFeatures=5, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"suicide_risk\")\n",
    "\n",
    "result = selector.fit(final_data).transform(final_data)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (6,[1],[-0.45271270012262355])\n",
      "Intercept: -0.5607334661895695\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='suicide_risk',maxIter=100, regParam=0.2, elasticNetParam=0.7)\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_data)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "--------------------Logistic Regression model--------------------\n",
      "Logistic Regression model accuracy: 68.50%\n",
      "areaUnderROC of LR: 73.73%\n",
      "areaUnderPR of LR: 70.01%\n"
     ]
    }
   ],
   "source": [
    "lrModel_predictions = lrModel.transform(test_data)\n",
    "# Let's import the evaluator.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error. \n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"suicide_risk\", \n",
    "                                                  predictionCol=\"prediction\",\n",
    "                                                  metricName=\"accuracy\")\n",
    "lrModel_acc = acc_evaluator.evaluate(lrModel_predictions)\n",
    "\n",
    "# Let's start off with binary classification.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Note that the label column isn't named label, it's named PrivateIndex in this case.\n",
    "my_binary_eval = BinaryClassificationEvaluator(labelCol = 'suicide_risk')\n",
    "# Let's do something a bit more complex in terms of printing, just so it's formatted nicer. \n",
    "print(\"Here are the results!\")\n",
    "print('-'*20 + 'Logistic Regression model' + '-'*20)\n",
    "print('Logistic Regression model accuracy: {0:2.2f}%'.format(lrModel_acc*100))\n",
    "print('areaUnderROC of LR: {0:2.2f}%'.format(my_binary_eval.evaluate(lrModel_predictions)*100))\n",
    "print('areaUnderPR of LR: {0:2.2f}%'.format(my_binary_eval.\n",
    "                            evaluate(lrModel_predictions, \n",
    "                            {my_binary_eval.metricName: \"areaUnderPR\"})*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "--------------------Decision Tree model--------------------\n",
      "Decision Tree model accuracy: 91.13%\n",
      "areaUnderROC of DT: 95.50%\n",
      "areaUnderPR of DT: 91.04%\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(labelCol='suicide_risk',featuresCol='features',maxBins=120,maxDepth=5, \n",
    "                             impurity='gini')\n",
    "dtc_model = dtc.fit(train_data)\n",
    "dtc_model.featureImportances\n",
    "# print(dtc_model.toDebugString)\n",
    "dtc_predictions = dtc_model.transform(test_data)\n",
    "# Let's import the evaluator.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error. \n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"suicide_risk\", predictionCol=\"prediction\", \n",
    "                                                  metricName=\"accuracy\")\n",
    "dtcModel_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "# Let's start off with binary classification.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Note that the label column isn't named label, it's named PrivateIndex in this case.\n",
    "my_binary_eval = BinaryClassificationEvaluator(labelCol = 'suicide_risk')\n",
    "\n",
    "# Let's do something a bit more complex in terms of printing, just so it's formatted nicer. \n",
    "print(\"Here are the results!\")\n",
    "print('-'*20 + 'Decision Tree model' + '-'*20)\n",
    "print('Decision Tree model accuracy: {0:2.2f}%'.format(dtcModel_acc*100))\n",
    "print('areaUnderROC of DT: {0:2.2f}%'.format(my_binary_eval.evaluate(dtc_predictions)*100))\n",
    "print('areaUnderPR of DT: {0:2.2f}%'.format(my_binary_eval.\n",
    "                            evaluate(dtc_predictions, \n",
    "                            {my_binary_eval.metricName: \"areaUnderPR\"})*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(6, {0: 0.3718, 1: 0.2501, 2: 0.0234, 3: 0.0097, 4: 0.2615, 5: 0.0834})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "--------------------Random Forest model--------------------\n",
      "Random Forest model accuracy: 93.61%\n",
      "areaUnderROC of RF: 98.08%\n",
      "areaUnderPR of RF: 96.32%\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(labelCol='suicide_risk',featuresCol='features',maxBins=120,\n",
    "                             maxDepth=30, impurity='gini',numTrees=30)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "rfc_model.featureImportances\n",
    "# print(rfc_model.toDebugString)\n",
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "# Let's import the evaluator.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error. \n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"suicide_risk\", predictionCol=\"prediction\", \n",
    "                                                  metricName=\"accuracy\")\n",
    "rfcModel_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "# Let's start off with binary classification.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Note that the label column isn't named label, it's named PrivateIndex in this case.\n",
    "my_binary_eval = BinaryClassificationEvaluator(labelCol = 'suicide_risk')\n",
    "\n",
    "# Let's do something a bit more complex in terms of printing, just so it's formatted nicer. \n",
    "print(\"Here are the results!\")\n",
    "print('-'*20 + 'Random Forest model' + '-'*20)\n",
    "print('Random Forest model accuracy: {0:2.2f}%'.format(rfcModel_acc*100))\n",
    "print('areaUnderROC of RF: {0:2.2f}%'.format(my_binary_eval.evaluate(rfc_predictions)*100))\n",
    "print('areaUnderPR of RF: {0:2.2f}%'.format(my_binary_eval.\n",
    "                            evaluate(rfc_predictions, \n",
    "                            {my_binary_eval.metricName: \"areaUnderPR\"})*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(6, {0: 0.3719, 1: 0.2918, 2: 0.0293, 3: 0.0168, 4: 0.1966, 5: 0.0936})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the relevant classifiers. \n",
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "dtc = DecisionTreeClassifier(labelCol='suicide_risk',featuresCol='features',maxBins=120)\n",
    "rfc = RandomForestClassifier(labelCol='suicide_risk',featuresCol='features',maxBins=120)\n",
    "gbt = GBTClassifier(labelCol='suicide_risk',featuresCol='features',maxBins=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(6, {0: 0.167, 1: 0.4472, 2: 0.2793, 3: 0.0341, 4: 0.0227, 5: 0.0497})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_model.featureImportances\n",
    "rfc_model.featureImportances\n",
    "gbt_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4ecf99d2a7575a034006) of depth 5 with 57 nodes\n",
      "  If (feature 0 in {1.0})\n",
      "   If (feature 1 in {0.0,1.0,3.0,5.0,6.0,8.0,9.0,11.0,12.0,13.0,14.0,15.0,17.0,18.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,30.0,31.0,32.0,34.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,45.0,46.0,47.0,49.0,50.0,51.0,53.0,54.0,55.0,56.0,58.0,59.0,60.0,62.0,63.0,64.0,65.0,66.0,67.0,73.0,76.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0})\n",
      "    If (feature 1 in {5.0,6.0,8.0,9.0,11.0,12.0,13.0,14.0,17.0,18.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,31.0,32.0,34.0,37.0,40.0,41.0,42.0,43.0,45.0,46.0,51.0,53.0,54.0,55.0,56.0,58.0,59.0,62.0,63.0,65.0,66.0,67.0,73.0,76.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,87.0,88.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0})\n",
      "     If (feature 1 in {5.0,6.0,8.0,9.0,11.0,12.0,13.0,14.0,17.0,18.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,32.0,37.0,40.0,41.0,43.0,45.0,53.0,54.0,55.0,56.0,58.0,59.0,62.0,63.0,65.0,66.0,67.0,73.0,76.0,78.0,79.0,80.0,81.0,82.0,83.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,100.0})\n",
      "      If (feature 1 in {5.0,6.0,9.0,11.0,12.0,14.0,17.0,18.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,32.0,37.0,40.0,41.0,43.0,45.0,54.0,56.0,58.0,59.0,62.0,65.0,66.0,67.0,73.0,76.0,79.0,80.0,81.0,82.0,83.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,100.0})\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 not in {5.0,6.0,9.0,11.0,12.0,14.0,17.0,18.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,32.0,37.0,40.0,41.0,43.0,45.0,54.0,56.0,58.0,59.0,62.0,65.0,66.0,67.0,73.0,76.0,79.0,80.0,81.0,82.0,83.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,100.0})\n",
      "       Predict: 0.0\n",
      "     Else (feature 1 not in {5.0,6.0,8.0,9.0,11.0,12.0,13.0,14.0,17.0,18.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,32.0,37.0,40.0,41.0,43.0,45.0,53.0,54.0,55.0,56.0,58.0,59.0,62.0,63.0,65.0,66.0,67.0,73.0,76.0,78.0,79.0,80.0,81.0,82.0,83.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,100.0})\n",
      "      If (feature 3 in {0.0,1.0,2.0,3.0,5.0})\n",
      "       Predict: 0.0\n",
      "      Else (feature 3 not in {0.0,1.0,2.0,3.0,5.0})\n",
      "       Predict: 0.0\n",
      "    Else (feature 1 not in {5.0,6.0,8.0,9.0,11.0,12.0,13.0,14.0,17.0,18.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,31.0,32.0,34.0,37.0,40.0,41.0,42.0,43.0,45.0,46.0,51.0,53.0,54.0,55.0,56.0,58.0,59.0,62.0,63.0,65.0,66.0,67.0,73.0,76.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,87.0,88.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0})\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,5.0})\n",
      "      If (feature 3 in {0.0,2.0,3.0,5.0})\n",
      "       Predict: 0.0\n",
      "      Else (feature 3 not in {0.0,2.0,3.0,5.0})\n",
      "       Predict: 0.0\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,5.0})\n",
      "      If (feature 1 in {0.0,38.0,39.0,60.0,85.0,89.0})\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 not in {0.0,38.0,39.0,60.0,85.0,89.0})\n",
      "       Predict: 1.0\n",
      "   Else (feature 1 not in {0.0,1.0,3.0,5.0,6.0,8.0,9.0,11.0,12.0,13.0,14.0,15.0,17.0,18.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,30.0,31.0,32.0,34.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,45.0,46.0,47.0,49.0,50.0,51.0,53.0,54.0,55.0,56.0,58.0,59.0,60.0,62.0,63.0,64.0,65.0,66.0,67.0,73.0,76.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0})\n",
      "    If (feature 3 in {0.0,2.0,3.0,5.0})\n",
      "     If (feature 3 in {0.0,2.0,5.0})\n",
      "      If (feature 1 in {2.0,4.0,7.0,10.0,19.0,27.0,29.0,33.0,36.0,44.0,48.0,52.0,61.0,68.0,69.0,70.0,71.0,72.0,74.0,75.0,77.0})\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 not in {2.0,4.0,7.0,10.0,19.0,27.0,29.0,33.0,36.0,44.0,48.0,52.0,61.0,68.0,69.0,70.0,71.0,72.0,74.0,75.0,77.0})\n",
      "       Predict: 0.0\n",
      "     Else (feature 3 not in {0.0,2.0,5.0})\n",
      "      If (feature 1 in {2.0,4.0,7.0,16.0,19.0,27.0,29.0,35.0,36.0,44.0,48.0,57.0,61.0,68.0,70.0,71.0,72.0,74.0,75.0,77.0,86.0})\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 not in {2.0,4.0,7.0,16.0,19.0,27.0,29.0,35.0,36.0,44.0,48.0,57.0,61.0,68.0,70.0,71.0,72.0,74.0,75.0,77.0,86.0})\n",
      "       Predict: 1.0\n",
      "    Else (feature 3 not in {0.0,2.0,3.0,5.0})\n",
      "     If (feature 1 in {4.0,16.0,33.0,35.0,48.0,57.0,70.0,86.0})\n",
      "      If (feature 4 <= 2.904662528E9)\n",
      "       Predict: 0.0\n",
      "      Else (feature 4 > 2.904662528E9)\n",
      "       Predict: 1.0\n",
      "     Else (feature 1 not in {4.0,16.0,33.0,35.0,48.0,57.0,70.0,86.0})\n",
      "      If (feature 2 in {0.0})\n",
      "       Predict: 1.0\n",
      "      Else (feature 2 not in {0.0})\n",
      "       Predict: 1.0\n",
      "  Else (feature 0 not in {1.0})\n",
      "   If (feature 1 in {6.0,12.0,13.0,17.0,18.0,20.0,22.0,23.0,37.0,39.0,40.0,43.0,45.0,51.0,53.0,54.0,55.0,56.0,58.0,62.0,63.0,65.0,67.0,73.0,76.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0})\n",
      "    If (feature 1 in {6.0,12.0,17.0,23.0,43.0,45.0,51.0,54.0,58.0,62.0,65.0,67.0,73.0,76.0,79.0,80.0,81.0,82.0,83.0,85.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0})\n",
      "     If (feature 2 in {0.0,1.0,2.0,3.0,5.0})\n",
      "      If (feature 1 in {6.0,12.0,17.0,23.0,43.0,45.0,54.0,58.0,62.0,65.0,67.0,73.0,76.0,79.0,80.0,81.0,82.0,83.0,87.0,88.0,90.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0})\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 not in {6.0,12.0,17.0,23.0,43.0,45.0,54.0,58.0,62.0,65.0,67.0,73.0,76.0,79.0,80.0,81.0,82.0,83.0,87.0,88.0,90.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0})\n",
      "       Predict: 0.0\n",
      "     Else (feature 2 not in {0.0,1.0,2.0,3.0,5.0})\n",
      "      If (feature 1 in {23.0,43.0,51.0,54.0,58.0,62.0,67.0,73.0,76.0,79.0,80.0,81.0,82.0,83.0,85.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,99.0,100.0})\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 not in {23.0,43.0,51.0,54.0,58.0,62.0,67.0,73.0,76.0,79.0,80.0,81.0,82.0,83.0,85.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,99.0,100.0})\n",
      "       Predict: 1.0\n",
      "    Else (feature 1 not in {6.0,12.0,17.0,23.0,43.0,45.0,51.0,54.0,58.0,62.0,65.0,67.0,73.0,76.0,79.0,80.0,81.0,82.0,83.0,85.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0})\n",
      "     If (feature 2 in {2.0,5.0})\n",
      "      If (feature 2 in {5.0})\n",
      "       Predict: 0.0\n",
      "      Else (feature 2 not in {5.0})\n",
      "       Predict: 0.0\n",
      "     Else (feature 2 not in {2.0,5.0})\n",
      "      If (feature 5 <= 834.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 5 > 834.0)\n",
      "       Predict: 0.0\n",
      "   Else (feature 1 not in {6.0,12.0,13.0,17.0,18.0,20.0,22.0,23.0,37.0,39.0,40.0,43.0,45.0,51.0,53.0,54.0,55.0,56.0,58.0,62.0,63.0,65.0,67.0,73.0,76.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0})\n",
      "    If (feature 2 in {5.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 2 not in {5.0})\n",
      "     If (feature 2 in {2.0})\n",
      "      If (feature 1 in {3.0,8.0,9.0,14.0,15.0,16.0,19.0,24.0,27.0,29.0,41.0,46.0,50.0,61.0,64.0,66.0,77.0})\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 not in {3.0,8.0,9.0,14.0,15.0,16.0,19.0,24.0,27.0,29.0,41.0,46.0,50.0,61.0,64.0,66.0,77.0})\n",
      "       Predict: 1.0\n",
      "     Else (feature 2 not in {2.0})\n",
      "      If (feature 1 in {8.0,15.0,34.0,46.0,77.0,86.0})\n",
      "       Predict: 1.0\n",
      "      Else (feature 1 not in {8.0,15.0,34.0,46.0,77.0,86.0})\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dtc_model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_predictions = dtc_model.transform(test_data)\n",
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start off with binary classification.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Note that the label column isn't named label, it's named PrivateIndex in this case.\n",
    "my_binary_eval = BinaryClassificationEvaluator(labelCol = 'suicide_risk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC\n",
      "0.9534551410912341\n",
      "RFC\n",
      "0.9666498849704965\n",
      "GBT\n",
      "0.9068846681285399\n"
     ]
    }
   ],
   "source": [
    "# This is the area under the curve. This indicates that the data is highly seperable.\n",
    "print(\"DTC\")\n",
    "print(my_binary_eval.evaluate(dtc_predictions))\n",
    "\n",
    "# RFC improves accuracy but also model complexity. RFC outperforms DTC in nearly every situation.\n",
    "print(\"RFC\")\n",
    "print(my_binary_eval.evaluate(rfc_predictions))\n",
    "\n",
    "# We can't repeat these exact steps for GBT. If you print the schema of all three, you may be able to notice why.\n",
    "# Instead, let's redefine the object:\n",
    "my_binary_gbt_eval = BinaryClassificationEvaluator(labelCol='suicide_risk', rawPredictionCol='prediction')\n",
    "print(\"GBT\")\n",
    "print(my_binary_gbt_eval.evaluate(gbt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "----------------------------------------\n",
      "A single decision tree has an accuracy of: 91.50%\n",
      "----------------------------------------\n",
      "A random forest ensemble has an accuracy of: 90.89%\n",
      "----------------------------------------\n",
      "An ensemble using GBT has an accuracy of: 92.22%\n"
     ]
    }
   ],
   "source": [
    "# Let's import the evaluator.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error. \n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"suicide_risk\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "gbt_acc = acc_evaluator.evaluate(gbt_predictions)\n",
    "# Let's do something a bit more complex in terms of printing, just so it's formatted nicer. \n",
    "print(\"Here are the results!\")\n",
    "print('-'*40)\n",
    "print('A single decision tree has an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('-'*40)\n",
    "print('A random forest ensemble has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print('-'*40)\n",
    "print('An ensemble using GBT has an accuracy of: {0:2.2f}%'.format(gbt_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='risk_pre',\n",
    "                                       labelCol='suicide_risk')\n",
    "results.select('Survived','prediction').show()\n",
    "AUC = my_eval.evaluate(results)\n",
    "\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"features\" does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o204.fit.\n: java.lang.IllegalArgumentException: Field \"features\" does not exist.\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:264)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:264)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:263)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:40)\n\tat org.apache.spark.ml.feature.ChiSqSelector.transformSchema(ChiSqSelector.scala:182)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.ChiSqSelector.fit(ChiSqSelector.scala:159)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9e24405c3075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                          outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ChiSqSelector output with top %d features selected\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumTopFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"features\" does not exist.'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "selector = ChiSqSelector(numTopFeatures=8, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"risk\")\n",
    "\n",
    "result = selector.fit(data).transform(data)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country',\n",
       " 'year',\n",
       " 'sex',\n",
       " 'age',\n",
       " 'suicides/100k pop',\n",
       " 'HDI for year',\n",
       " ' gdp_for_year ($) ',\n",
       " 'gdp_per_capita ($)',\n",
       " 'generation']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country',\n",
       " 'year',\n",
       " 'sex',\n",
       " 'age',\n",
       " 'suicides_no',\n",
       " 'population',\n",
       " 'suicides/100k pop',\n",
       " 'country-year',\n",
       " 'HDI for year',\n",
       " ' gdp_for_year ($) ',\n",
       " 'gdp_per_capita ($)',\n",
       " 'generation']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+-----------+-----------+----------+-----------------+------------+------------+------------------+------------------+---------------+\n",
      "|country|year|   sex|        age|suicides_no|population|suicides/100k pop|country-year|HDI for year| gdp_for_year ($) |gdp_per_capita ($)|     generation|\n",
      "+-------+----+------+-----------+-----------+----------+-----------------+------------+------------+------------------+------------------+---------------+\n",
      "|Albania|1987|  male|15-24 years|         21|    312900|             6.71| Albania1987|        null|     2,156,624,900|               796|   Generation X|\n",
      "|Albania|1987|  male|35-54 years|         16|    308000|             5.19| Albania1987|        null|     2,156,624,900|               796|         Silent|\n",
      "|Albania|1987|female|15-24 years|         14|    289700|             4.83| Albania1987|        null|     2,156,624,900|               796|   Generation X|\n",
      "|Albania|1987|  male|  75+ years|          1|     21800|             4.59| Albania1987|        null|     2,156,624,900|               796|G.I. Generation|\n",
      "|Albania|1987|  male|25-34 years|          9|    274300|             3.28| Albania1987|        null|     2,156,624,900|               796|        Boomers|\n",
      "|Albania|1987|female|  75+ years|          1|     35600|             2.81| Albania1987|        null|     2,156,624,900|               796|G.I. Generation|\n",
      "|Albania|1987|female|35-54 years|          6|    278800|             2.15| Albania1987|        null|     2,156,624,900|               796|         Silent|\n",
      "|Albania|1987|female|25-34 years|          4|    257200|             1.56| Albania1987|        null|     2,156,624,900|               796|        Boomers|\n",
      "|Albania|1987|  male|55-74 years|          1|    137500|             0.73| Albania1987|        null|     2,156,624,900|               796|G.I. Generation|\n",
      "|Albania|1987|female|05-14 years|          0|    311000|              0.0| Albania1987|        null|     2,156,624,900|               796|   Generation X|\n",
      "|Albania|1987|female|55-74 years|          0|    144600|              0.0| Albania1987|        null|     2,156,624,900|               796|G.I. Generation|\n",
      "|Albania|1987|  male|05-14 years|          0|    338200|              0.0| Albania1987|        null|     2,156,624,900|               796|   Generation X|\n",
      "|Albania|1988|female|  75+ years|          2|     36400|             5.49| Albania1988|        null|     2,126,000,000|               769|G.I. Generation|\n",
      "|Albania|1988|  male|15-24 years|         17|    319200|             5.33| Albania1988|        null|     2,126,000,000|               769|   Generation X|\n",
      "|Albania|1988|  male|  75+ years|          1|     22300|             4.48| Albania1988|        null|     2,126,000,000|               769|G.I. Generation|\n",
      "|Albania|1988|  male|35-54 years|         14|    314100|             4.46| Albania1988|        null|     2,126,000,000|               769|         Silent|\n",
      "|Albania|1988|  male|55-74 years|          4|    140200|             2.85| Albania1988|        null|     2,126,000,000|               769|G.I. Generation|\n",
      "|Albania|1988|female|15-24 years|          8|    295600|             2.71| Albania1988|        null|     2,126,000,000|               769|   Generation X|\n",
      "|Albania|1988|female|55-74 years|          3|    147500|             2.03| Albania1988|        null|     2,126,000,000|               769|G.I. Generation|\n",
      "|Albania|1988|female|25-34 years|          5|    262400|             1.91| Albania1988|        null|     2,126,000,000|               769|        Boomers|\n",
      "+-------+----+------+-----------+-----------+----------+-----------------+------------+------------+------------------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see the data. You'll notice nulls.\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few things we need to do before Spark can accept the data!\n",
    "# It needs to be in the form of two columns: \"label\" and \"features\".\n",
    "\n",
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country',\n",
       " 'year',\n",
       " 'sex',\n",
       " 'age',\n",
       " 'suicides_no',\n",
       " 'population',\n",
       " 'suicides/100k pop',\n",
       " 'country-year',\n",
       " 'HDI for year',\n",
       " ' gdp_for_year ($) ',\n",
       " 'gdp_per_capita ($)',\n",
       " 'generation']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's visualise the columns to help with assembly. \n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/cb/28/91f26bd088ce8e22169032100d4260614fc3da435025ff389ef1d396a433/pip-20.2.4-py2.py3-none-any.whl (1.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 914kB/s eta 0:00:01  4% |█▎                              | 61kB 12.5MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "Successfully installed pip-20.2.4\n",
      "\u001b[33mDEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.9.1-py2.py3-none-any.whl (216 kB)\n",
      "\u001b[K     |████████████████████████████████| 216 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.17.1 in ./.local/lib/python3.5/site-packages (from seaborn) (0.23.4)\n",
      "Collecting scipy>=0.17.1\n",
      "  Downloading scipy-1.4.1-cp35-cp35m-manylinux1_x86_64.whl (26.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.0 MB 24.3 MB/s eta 0:00:01     |███████████████████████████▊    | 22.5 MB 24.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in ./.local/lib/python3.5/site-packages (from seaborn) (1.15.1)\n",
      "Requirement already satisfied: matplotlib>=1.5.3 in ./.local/lib/python3.5/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: pytz>=2011k in ./.local/lib/python3.5/site-packages (from pandas>=0.17.1->seaborn) (2018.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in ./.local/lib/python3.5/site-packages (from pandas>=0.17.1->seaborn) (2.7.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.local/lib/python3.5/site-packages (from matplotlib>=1.5.3->seaborn) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in ./.local/lib/python3.5/site-packages (from matplotlib>=1.5.3->seaborn) (2.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.5/site-packages (from matplotlib>=1.5.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied: six>=1.10 in ./.local/lib/python3.5/site-packages (from matplotlib>=1.5.3->seaborn) (1.11.0)\n",
      "Requirement already satisfied: setuptools in ./.local/lib/python3.5/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.3->seaborn) (39.2.0)\n",
      "Installing collected packages: scipy, seaborn\n",
      "Successfully installed scipy-1.4.1 seaborn-0.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pip\n",
    "!pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+------+-----------+------------------+------------------+------------------+\n",
      "|summary|   country|              year|   sex|        age|       suicides_no|        population| suicides/100k pop|\n",
      "+-------+----------+------------------+------+-----------+------------------+------------------+------------------+\n",
      "|  count|     27820|             27820| 27820|      27820|             27820|             27820|             27820|\n",
      "|   mean|      null|2001.2583752695903|  null|       null|242.57440690150972|1844793.6173975556|12.816097411933894|\n",
      "| stddev|      null| 8.469055024441408|  null|       null| 902.0479168336403|3911779.4417563565| 18.96151101450326|\n",
      "|    min|   Albania|              1985|female|05-14 years|                 0|               278|               0.0|\n",
      "|    max|Uzbekistan|              2016|  male|  75+ years|             22338|          43805214|            224.97|\n",
      "+-------+----------+------------------+------+-----------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can use the describe method get some general statistics on our data too. Remember to show the DataFrame!\n",
    "# But what about data type?\n",
    "data.describe(['country',\n",
    " 'year',\n",
    " 'sex',\n",
    " 'age',\n",
    " 'suicides_no',\n",
    " 'population',\n",
    " 'suicides/100k pop',\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------------+------------------+------------------+----------+\n",
      "|summary|  country-year|       HDI for year| gdp_for_year ($) |gdp_per_capita ($)|generation|\n",
      "+-------+--------------+-------------------+------------------+------------------+----------+\n",
      "|  count|         27820|               8364|             27820|             27820|     27820|\n",
      "|   mean|          null| 0.7766011477761785|              null|16866.464414090584|      null|\n",
      "| stddev|          null|0.09336670859029984|              null|18887.576472205576|      null|\n",
      "|    min|   Albania1987|              0.483| 1,002,219,052,968|               251|   Boomers|\n",
      "|    max|Uzbekistan2014|              0.944|       997,007,926|            126352|    Silent|\n",
      "+-------+--------------+-------------------+------------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe([\n",
    " 'country-year',\n",
    " 'HDI for year',\n",
    " ' gdp_for_year ($) ',\n",
    " 'gdp_per_capita ($)',\n",
    " 'generation']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
